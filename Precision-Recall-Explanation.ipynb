{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Precision/Recall Explanation\n",
    "\n",
    "Precision/Recall is a measure of how precise a model can predict the truth. It is used widely in statistics.\n",
    "\n",
    "While there are many different ways to explain it, including this excellent article on [Medium](https://towardsdatascience.com/accuracy-precision-recall-or-f1-331fb37c5cb9), I've never found a simple and intuitive explanation, so I thought I would post it here, along with some code, so you can play around with it and try it out.\n",
    "\n",
    "Run this code below:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "     class 0       0.50      1.00      0.67         1\n",
      "     class 1       0.00      0.00      0.00         1\n",
      "     class 2       1.00      0.67      0.80         3\n",
      "\n",
      "   micro avg       0.60      0.60      0.60         5\n",
      "   macro avg       0.50      0.56      0.49         5\n",
      "weighted avg       0.70      0.60      0.61         5\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "y_true = [0, 1, 2, 2, 2]\n",
    "y_predicted = [0, 0, 2, 2, 1]\n",
    "target_names = ['class 0', 'class 1', 'class 2']\n",
    "print(classification_report(y_true, y_predicted, target_names=target_names))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Precision\n",
    "\n",
    "Is telling us how precise our model is. \n",
    "\n",
    "In the example above for `0` (or `class 0`) we have the precision of `0.50`. This means that in `50%` of cases our model predicted the right value correctly.\n",
    "\n",
    "The value is `0.00` for `1` (or `class 1`) and that is because `none` of the times when we predicted the value is `1` it was true. So our model is not precise at all for predicting `1`. And even if it does it, it does it wrongly.\n",
    "\n",
    "The value is `1.00` for `2`. This means that in `100%` of cases when our model predicted the value was `2` it was true. So the prediction of `2` was always precise, every time our model said the value was `2`, it was true.\n",
    "\n",
    "\n",
    "## Recall\n",
    "\n",
    "Recall helps us see the percentage of truth that we actually get. Our model may be quite precise in guessing, that is, when it guesses, it may do it right. But it might also miss a lot of values in the original set.\n",
    "\n",
    "In our case for `0` we have the recall measure of `1`, which means that our model managed to guess all the `0` values in the original set. However, the measure of *precision* above was `0.50`, which means that while our model guess all the `0` values, it also had some false positives, so it wasn't very precise.\n",
    "\n",
    "For `1` we have the recall measure of `0`, which means our model didn't guess any `1` values from the original set. And as the *precision* is also `0`, even when we did guess, it wasn't right.\n",
    "\n",
    "For `2` we have the recall measure of `0.67`, which means that we managed to get `2/3` of all the `2` values in the original set. The precision was `1`, so every time we guessed, we were right.\n",
    "\n",
    "\n",
    "## F1 Score\n",
    "\n",
    "F1 score is a compound of both precision and recall, helping us find the balance between the two. The formula is \n",
    "\n",
    "`F1 = 2 * precision * recall / (precision + recall)`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test\n",
    "\n",
    "Let's do a test and try create a different model to see if we can interpret the results well:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_true = ['true','false','true','false','false']\n",
    "x_predicted = ['true','true','false','false','false']\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this case the results we get should be:\n",
    "\n",
    "**For `true`** \n",
    "precision `0.5` because of the 2 times we guessed, only half of the time was true. \n",
    "recall `0.5` because we only got it right 1 out of 2 times.\n",
    "\n",
    "**For `false`**\n",
    "precision `0.67` because of the 3 times we guessed, we were right 2 times (2/3)\n",
    "recall `0.67` because we got 2 values right out of 3 total\n",
    "\n",
    "Let's check:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "       false       0.67      0.67      0.67         3\n",
      "        true       0.50      0.50      0.50         2\n",
      "\n",
      "   micro avg       0.60      0.60      0.60         5\n",
      "   macro avg       0.58      0.58      0.58         5\n",
      "weighted avg       0.60      0.60      0.60         5\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(x_true, x_predicted))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Which means our model is better in predicting `false` than `true`, which it gets right as much as we get it right randomly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
